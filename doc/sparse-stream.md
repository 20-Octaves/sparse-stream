# Sparse Stream

This document describes the **Sparse Stream** format.
Sparse Stream is a meta format that emerged from our efforts to formalize, simplify, and improve a low-level stream implementation used with a generator of high-resoloution, high-dimensinality acoustic features.

## Motivation

We needed a format for streaming acoustic features that would be simple, space efficient, supporting sparse, high-dimensional features, generated by multiple, simultaneous high-resolution acoustic-analysis alogrithms, and readily extensible as new feature algorithms are developed.

A practical, low-level format and protocol was developed first, the simple, abstract structure emerged while this documentation was being developed.

## Meta format

Sparse Stream is a meta format that defines a simple structure for concrete implementations which produce a set of acoustic features derived from an audio (stream) input.
The stream of a sparse Stream implementation represents the output of an acoustic-feature generator in response to an audio input that it has processed.

For example, given a stero audio file as input, a feature generator will generate a stream of bytes (likely saved to a file) with acoustic feature values derived from the stereo input.
For a well-designed feature generator, the contents of the stream (or file) file will be directly useful for feature-consuming tasks such as machine learning, acoustic object or scene identifiation, or visualization of the input.

For a live microphone input, the streaming format allows acoustic features to be made available to downstream feature-consuming applications with the low latency needed for real-time response.
The format supports high-precision acoustic-feature generators for which the features are emitted at a higher rate than the audio frame-rate samplerate.

The Sparse Stream format aims to be relatively un-opinionated about how an implementation encodes the features it represents.
It provides a simple, three-level framework that addresses some fundamental constraints of temporally-based acoustic-feature streams:
- The flow of data in a stream may occasionally be interupted and unknown amounts of data may be lost before the underlying transport recovers and the flow of data resumes -- and the stream producer may be unaware of the data loss.  The format support straightforward reframing recovery from such damage to the stream.
- The range and precision of the temporal index of data and features (timestamp) in a stream can be very large, e.g. from microseconds to weeks.  The format supports efficient factoring of timestamps.
- Generators of high-precision features can asynchronously emit numerous features for each audio frame.  The format supports this low-level requirement.
- Annotations (non-feature data) are useful though not essential

The format is somewhat biased about being close to the source of audio data.
As such, it readily accommodates audio that has a single fixed samplerate, a fixed number of audio channels, and frames which interleave the samples from each channel.

Overall there is an implicit bias towards front-end devices with microphones (on the edge, and with computes and bandwidth to spare) doing some relatively expensive up-front application-specific computation, and either using the feature stream directly or sending it to more central services.

### Notation

For readability of large numbers in the examples, we adopt the use of underscores in numeric literals, as do [many modern programming languages](https://peps.python.org/pep-0515/#prior-art).
E.g. one thousand as `1_000`, and two raised to the twentieth power as `1_048_576`.


## Events

A Sparse Stream encodes a set of events, where each event feature is a single "point" in a (potentially large) multi-dimensional acoustic-feature space.
These analysis dimensions typically correspond directly to feature detectors and their outputs.

Consider 2-channel audio being processed by a front-end that has 7 different types of detectors (feature generators).
One such detector type (number 5) could indicate the discretized energy output of a set of 50 filters.
At frame 19_364 for the left audio channel, detector type 5, filter number 23 is detecting a level of 47.
Such a point could be represented by the tuple `(19_364, 0, 5, 23, 47)`.
The overall size of the space in which this point appears might be `(65_536, 2, 7, 50, 100)`.

Based on the details of the generator and the audio input, the dimensions of detector type, energy, filter number, and audio channel would each have a well-defined set of index values.
For instance, there might be 2 audio channels, and the front end has 7 detector types, and detector type 5 has 100 distinct energy values, and an array of 50 such filters.

In general, for a given feature generator, the set of analysis dimensions and the number of indices in each dimension are configuration parameters of the generator.
The set of dimensions and the number of each of their indicies are (relatively) small and fixed for the duration of the stream.
E.g. a typical generator may have 7 detection algorithms, each generating 2 feature values when triggered, arranged in an array of 1_600 detectors -- so, a 22_400 dimensional space (not including the temporal index)

### The unbounded dimension

One of the dimensions of such an event represents a "temporal" index, e.g. the frame number of the audio samples.
As such, the index for this temporal dimension is (monotonically) increasing with the stream's events and is approached differently than the other (much) smaller dimensions.
This distinguished temporal dimension gets special (opinionated) accommodation in the Sparse Stream format.

## Syntax

A Sparse Stream is a sequence of terminal objects.
The "syntax" of the objects in the stream provie a simple hierachy of containers for the events.
This hierachy supports an efficient (stream size) representation of the events by grouping them so that the items in a group share the same temporal index.

We use an [EBNF-based](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form) description of the syntax of the abstract objects in a stream.

### Terminals

There are six abstract terminals:
- SENTINEL -- used to start a stream, and used for "heartbeat" to support alignment and re-starting on streams that may have uncontrolled, unrecoverable gaps, e.g. due to an unreliable data link
- CONFIG -- configuration information about the structure of the acoustic-feature points (types, dimensions, sizes) and about the audio input
- MOMENT -- supports rarely-occuring, large-scale increments of the distinguished temporal index and related bookkeeping
- ANNOTATION -- generator-specific meta features and pertinent information
- TICK -- finer-grained details about the particular frame, audio channel, and sample
- FEATURES -- details for an individual feature point
- END -- end of stream

The details of the semantics and how these terminals are represented in the stream depends upon the particular implementation.
One possible pattern is to use fixed size objects, e.g. 8 bytes each, where the value of the first byte indicates the terminal type, and the remaining bytes encode the "value" of the terminal.

### Language

The Sparse Stream language rules specify a stream as a sequence of nested containments:

```
stream = 1* epoch , END .
epoch  = SENTINEL , 1* CONFIG , * moment .
moment = MOMENT , * ( ANNOTATION | tick ) .
tick   = TICK , * FEATURES .
```

For instance here is a valid stream, with indentation to illustrate the containment structure:

```
SENTINEL CONFIG CONFIG
  MOMENT
    TICK
      FEATURES
      FEATURES
    TICK
    TICK
      FEATURES
  MOMENT
    ANNOTATION
    TICK
      FEATURES
      FEATURES
SENTINEL CONFIG CONFIG
  MOMENT
    TICK
      FEATURES
      FEATURES
END
```

In practice, for a high-precision generator, each TICK will correspond to a single audio sample, and dozens of FEATURES are typically emitted for each TICK, and thousands of TICKs appear in each MOMENT.
The generation of multiple FEATURES points for each TICK sample means that the resulting stream will be (much) larger than the audio input.

## Concrete example

#### **`Octv, version 1`**

Here are essential details of a C language implementation that conforms to the Sparse Stream meta specification.
While detailed, and actually implemented at some point, the [examples below](#examples) are non-normative.

Each terminal is an 8 byte structure, and the first byte of each indicates the terminal type.
The remaining bytes provide the value for the terminal.
A union of all terminal structs, with an anoymous type field, is used for writing and reading.
Upon read, the anonymous type field is used for dispatch to the logic for the specific terminal type.

Except where otherwise noted, the descriptions of the terminals are specific to version 1 of Octv.

### SENTINEL

```
typedef struct {
  char type;
  char chars[3];
  uint8_t bytes[4];
} OctvSentinel;
```

The SENTINEL type is 79 (0x4f), the character code of "O".
The SENTINEL terminal is fixed -- all 8 bytes are specified and will be the same for all versions of Octv.
```
static const OctvSentinel sentinel = { 'O', 'c', 't', 'v', 0xa4, 0x6d, 0xae, 0xb6 };
```
The SENTINEL identifies the stream as an Octv stream, and makes recovery from dropped data very straightforward.


### END

```
typedef struct {
  char type;
  char chars[3];
  uint8_t bytes[4];
} OctvEnd;
```

The END type is 69 (0x45), the character code of "E".
All 8 bytes of the END terminal are specified:
```
static const OctvEnd end = { 'E', 'n', 'd', ' ', 0xa4, 0x6d, 0xae, 0xb6 };
```

### CONFIG

```
typedef struct {
  uint8_t type;
  uint8_t version;

  uint8_t num_audio_channels;
  uint8_t audio_sample_rate[3];

  uint16_t num_detectors_max;
} OctvConfig;
```

E.g.
```
int samplerate = 48000;
OctvConfig config = { 1, 1,
                      2,
                      samplerate & 0xff, (samplerate >> 8) & 0xff, (samplerate >> 16) & 0xff,
                      1440 };
```


The `type` field is 1, indicating a CONFIG terminal.
For all versions of Octv, a CONFIG of type 1 is always required immediately following the SENTINEL.

The intent is that future versions of the concrete Octv format will always have the same SENTINEL (and END) terminals, and the version field of the CONFIG terminal will be used to dispatch to version-specific logic for the stream.

The `version` is 1 in the examples here.

#### Audio

The `num_audio_channels` field specifies how many channels of concurrent audio are represented in the stream, typically between 1 and 4.
For version 1, the 24-bit `audio_sample_rate` integer encodes samplerate of the stream in Hz.

#### Feature generators

The `num_detectors_max` field indicates the largest size of the sets of detectors implemented in the system, e.g. a maximum array size of 1_440 configured detection modules.
See `type` and `detector_index` in FEATURE below.


### MOMENT
```
typedef struct {
  uint8_t type;
  uint8_t _reserved[3];

  uint32_t frame_index_base;
} OctvMoment;
```

Version 1 of Octv uses 48 bits for the index of the distinguished, temporal dimension.
This allows a single stream to span more than 8 months duration of a (typical) 48_000 Hz audio signal.
In practice, we have worked with streams of up to several days duration, but rarely more than a week.

The `frame_index_base` field is the high-order 32 bits of the 48-bit frame counter.
This terminal is emitted whenever the lowest 16 bits of the frame counter are 0.
E.g. at the start of work on a frame:
```
++frame_counter;
if (!(frame_counter & ((1 << 16) - 1))) {
   OctvMoment moment = { 2, 0, 0, 0, frame_counter >> 16 };
   emit(&moment);
}
```

For a 48_000 Hz signal a MOMENT terminal is emitted every 1.365 seconds.

### ANNOTATION

Annotations are used to indicate coarse-grained, non-feature events that may be of interest to stream consumers.
Annotations themselves do not change the semantics of TICK and FEATURE events.
It should be acceptable to filter ANNOTATIONS out of a stream -- that is, their presence should not significantly impact downstream application performance.

For example, a run of frames with the same sample value, usually 0, may be routinely observed in some settings (microphone input from web browsers).
Such runs could be indicated with the following struct.
```
typedef struct {
  uint8_t type;
  uint8_t _reserved[3];
  uint16_t audio_frame_index_first;
  uint16_t run_duration;
} OctvAnnotation;
```
This might be useful in some GUI settings.
But, a machine learning system should have these events labeled in the training data and would (quickly) learn their distinctive representation in the emitted features, and so would not need to use these annotations.

Annotations are somewhat akin to comments -- use them judiciously:
If a particular annotation type starts being used to convey feature-like information, its semantics should be moved to a new FEATURE type.


### TICK
```
typedef struct {
  uint8_t type;
  uint8_t audio_channel;
  uint16_t audio_frame_index;
  float audio_sample;
} OctvTick;
```

The TICK terminal indicates that features will be emitted for the `audio_sample` at frame `audio_frame_index`, channel `audio_channel`.
The `float` samples typically have a range of 1 to -1.

### FEATURE
```
typedef struct {
  uint8_t type;

  uint8_t frame_offset;

  int8_t level[4];

  uint16_t detector_index;
} OctvFeature;
```

The FEATURE terminal encompases several distinct types of acoustic feature specific to Octv, version 1.

Each FEATURE `type` indicates a particular feature-generating algorithm.

The `detector_index` indicates which of the array detectors (implementing the algorithm) has generated the feature.

The four `level` values are used to encode the detector outputs, e.g. feature levels.
In practice to date the values have ranged from -31 to +31 for most feature types.
And only rarely does an algorithm use all four slots.

For high-precision features, the `frame_offset` encodes a fractional measure of the frame duration (e.g. how many sixteenths), indicating when, between the previous and current frames, the feature was observed.

The details behind `detector_index`, `type`, and `level` are beyond the scope of this documentation.
In some ways these details are irrelevant anyway due to the ubiquitous use of machine learning to discover patterns in sets of features and to assess feature importance scores -- that is, the importance of these details depends on the application being learned rather than descriptions provided here.

## Examples
